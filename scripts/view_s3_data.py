#!/usr/bin/env python3
"""
æŸ¥çœ‹ S3 æ•°æ®è„šæœ¬
ç”¨äºè¯Šæ–­ dashboard ä¸ºä»€ä¹ˆæ²¡æœ‰æ•°æ®

ç”¨æ³•:
    python scripts/view_s3_data.py                    # åˆ—å‡ºæ‰€æœ‰æ–‡ä»¶
    python scripts/view_s3_data.py --date 2026-01-26  # æŸ¥çœ‹ç‰¹å®šæ—¥æœŸ
    python scripts/view_s3_data.py --check-dashboard # æ£€æŸ¥dashboardéœ€è¦çš„æ•°æ®
"""
import os
import sys
import json
import argparse
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Any

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

try:
    import boto3
    from botocore.exceptions import ClientError, NoCredentialsError
    AWS_AVAILABLE = True
except ImportError:
    print("âŒ boto3 not installed. Install with: pip install boto3")
    sys.exit(1)

def format_size(size_bytes: int) -> str:
    """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
    for unit in ['B', 'KB', 'MB', 'GB']:
        if size_bytes < 1024.0:
            return f"{size_bytes:.2f} {unit}"
        size_bytes /= 1024.0
    return f"{size_bytes:.2f} TB"

def list_s3_objects(s3_client, bucket: str, prefix: str = "", max_keys: int = 1000) -> List[Dict]:
    """åˆ—å‡ºS3å¯¹è±¡"""
    objects = []
    try:
        paginator = s3_client.get_paginator('list_objects_v2')
        pages = paginator.paginate(Bucket=bucket, Prefix=prefix, MaxKeys=max_keys)
        
        for page in pages:
            if 'Contents' not in page:
                continue
            for obj in page['Contents']:
                objects.append({
                    'key': obj['Key'],
                    'size': obj['Size'],
                    'last_modified': obj['LastModified']
                })
    except Exception as e:
        print(f"âŒ Error listing objects: {e}")
    
    return objects

def read_s3_object(s3_client, bucket: str, key: str) -> Any:
    """è¯»å–S3å¯¹è±¡å†…å®¹"""
    try:
        response = s3_client.get_object(Bucket=bucket, Key=key)
        content = response['Body'].read().decode('utf-8')
        return json.loads(content)
    except ClientError as e:
        error_code = e.response.get('Error', {}).get('Code', '')
        if error_code == 'NoSuchKey':
            return None
        raise
    except json.JSONDecodeError:
        return content  # è¿”å›åŸå§‹å†…å®¹å¦‚æœä¸æ˜¯JSON

def check_dashboard_data(s3_client, bucket: str, date: str):
    """æ£€æŸ¥dashboardéœ€è¦çš„æ•°æ®æ–‡ä»¶"""
    print("\n" + "=" * 80)
    print(f"ğŸ” æ£€æŸ¥ Dashboard æ•°æ® (æ—¥æœŸ: {date})")
    print("=" * 80)
    
    # éœ€è¦æ£€æŸ¥çš„æ–‡ä»¶
    required_files = {
        'compute_signals': f"processed-data/{date}/compute-signals.json",
        'learning_signals': f"processed-data/{date}/learning-signals.json",
    }
    
    # æ£€æŸ¥åŸå§‹æ•°æ®
    raw_data_prefix = f"raw-data/{date}/"
    raw_objects = list_s3_objects(s3_client, bucket, raw_data_prefix)
    
    print(f"\nğŸ“Š åŸå§‹æ•°æ® (raw-data/{date}/):")
    if raw_objects:
        print(f"   âœ… æ‰¾åˆ° {len(raw_objects)} ä¸ªæ–‡ä»¶")
        # æŒ‰tickeråˆ†ç»„
        tickers = set()
        for obj in raw_objects:
            parts = obj['key'].split('/')
            if len(parts) >= 3:
                ticker = parts[2]
                tickers.add(ticker)
        print(f"   ğŸ“ˆ Tickers: {', '.join(sorted(tickers))}")
        print(f"   ğŸ“ ç¤ºä¾‹æ–‡ä»¶:")
        for obj in raw_objects[:5]:
            print(f"      - {obj['key']} ({format_size(obj['size'])})")
        if len(raw_objects) > 5:
            print(f"      ... è¿˜æœ‰ {len(raw_objects) - 5} ä¸ªæ–‡ä»¶")
    else:
        print(f"   âŒ æ²¡æœ‰æ‰¾åˆ°åŸå§‹æ•°æ®")
        print(f"   ğŸ’¡ æç¤º: éœ€è¦å…ˆè¿è¡Œæ•°æ®é‡‡é›†å™¨æ”¶é›†åŸå§‹æ•°æ®")
    
    # æ£€æŸ¥å¤„ç†åçš„æ•°æ®
    print(f"\nğŸ“Š å¤„ç†åçš„æ•°æ®:")
    for file_type, key in required_files.items():
        print(f"\n   {file_type.replace('_', ' ').title()}:")
        print(f"   Key: {key}")
        
        try:
            data = read_s3_object(s3_client, bucket, key)
            if data is None:
                print(f"   âŒ æ–‡ä»¶ä¸å­˜åœ¨")
            else:
                print(f"   âœ… æ–‡ä»¶å­˜åœ¨")
                
                if file_type == 'compute_signals':
                    signals = data.get('signals', [])
                    print(f"   ğŸ“ˆ Signalsæ•°é‡: {len(signals)}")
                    if signals:
                        tickers = [s.get('ticker') for s in signals]
                        print(f"   ğŸ“Š Tickers: {', '.join(tickers)}")
                        print(f"   ğŸ“„ ç¤ºä¾‹æ•°æ®:")
                        print(f"      {json.dumps(signals[0], indent=6, ensure_ascii=False)}")
                
                elif file_type == 'learning_signals':
                    models = data.get('models', {})
                    print(f"   ğŸ“ˆ Modelsæ•°é‡: {len(models)}")
                    if models:
                        print(f"   ğŸ“Š Tickers: {', '.join(models.keys())}")
                        print(f"   ğŸ“„ ç¤ºä¾‹æ•°æ® (ç¬¬ä¸€ä¸ªticker):")
                        first_ticker = list(models.keys())[0]
                        print(f"      {json.dumps(models[first_ticker], indent=6, ensure_ascii=False)}")
                
                # æ˜¾ç¤ºæ–‡ä»¶å¤§å°
                obj_info = next((o for o in list_s3_objects(s3_client, bucket, key) if o['key'] == key), None)
                if obj_info:
                    print(f"   ğŸ’¾ æ–‡ä»¶å¤§å°: {format_size(obj_info['size'])}")
                    print(f"   ğŸ• æœ€åä¿®æ”¹: {obj_info['last_modified']}")
        
        except Exception as e:
            print(f"   âŒ è¯»å–é”™è¯¯: {e}")
    
    # æ€»ç»“
    print("\n" + "=" * 80)
    print("ğŸ“‹ è¯Šæ–­æ€»ç»“")
    print("=" * 80)
    
    compute_data = read_s3_object(s3_client, bucket, required_files['compute_signals'])
    learning_data = read_s3_object(s3_client, bucket, required_files['learning_signals'])
    
    if not raw_objects:
        print("âŒ é—®é¢˜: æ²¡æœ‰åŸå§‹æ•°æ®")
        print("   ğŸ’¡ è§£å†³æ–¹æ¡ˆ: å¯åŠ¨æ•°æ®é‡‡é›†å™¨æ”¶é›†åŸå§‹æ•°æ®")
    elif compute_data is None:
        print("âŒ é—®é¢˜: Compute Agent è¿˜æ²¡æœ‰è¿è¡Œæˆ–å¤±è´¥")
        print("   ğŸ’¡ è§£å†³æ–¹æ¡ˆ: æ£€æŸ¥ Lambda å‡½æ•°æ˜¯å¦æ­£å¸¸è¿è¡Œ")
        print("   ğŸ’¡ æ‰‹åŠ¨è§¦å‘: åœ¨ AWS Console ä¸­æ‰‹åŠ¨è§¦å‘ Compute Agent Lambda")
    elif learning_data is None:
        print("âš ï¸  è­¦å‘Š: Learning Agent è¿˜æ²¡æœ‰è¿è¡Œ")
        print("   ğŸ’¡ è§£å†³æ–¹æ¡ˆ: Learning Agent æ¯å°æ—¶è¿è¡Œä¸€æ¬¡ï¼Œè¯·ç­‰å¾…")
    else:
        print("âœ… æ‰€æœ‰å¿…éœ€çš„æ•°æ®æ–‡ä»¶éƒ½å­˜åœ¨")
        print("   ğŸ’¡ å¦‚æœ dashboard ä»ç„¶æ²¡æœ‰æ•°æ®ï¼Œæ£€æŸ¥:")
        print("      1. API ç«¯ç‚¹æ˜¯å¦æ­£å¸¸å·¥ä½œ")
        print("      2. æµè§ˆå™¨æ§åˆ¶å°æ˜¯å¦æœ‰é”™è¯¯")
        print("      3. ç½‘ç»œè¯·æ±‚æ˜¯å¦æˆåŠŸ")

def list_all_data(s3_client, bucket: str, prefix: str = ""):
    """åˆ—å‡ºæ‰€æœ‰æ•°æ®"""
    print("\n" + "=" * 80)
    print(f"ğŸ“‚ S3 Bucket: {bucket}")
    print(f"ğŸ“ Prefix: {prefix or '(å…¨éƒ¨)'}")
    print("=" * 80)
    
    objects = list_s3_objects(s3_client, bucket, prefix)
    
    if not objects:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶")
        return
    
    print(f"\næ‰¾åˆ° {len(objects)} ä¸ªæ–‡ä»¶:\n")
    
    # æŒ‰å‰ç¼€åˆ†ç»„
    groups: Dict[str, List[Dict]] = {}
    for obj in objects:
        key = obj['key']
        # è·å–ç¬¬ä¸€çº§ç›®å½•
        first_dir = key.split('/')[0] if '/' in key else ''
        if first_dir not in groups:
            groups[first_dir] = []
        groups[first_dir].append(obj)
    
    # æ˜¾ç¤ºåˆ†ç»„
    for group_name, group_objects in sorted(groups.items()):
        print(f"ğŸ“ {group_name}/ ({len(group_objects)} ä¸ªæ–‡ä»¶)")
        for obj in sorted(group_objects, key=lambda x: x['key'])[:10]:
            size_str = format_size(obj['size'])
            mod_time = obj['last_modified'].strftime('%Y-%m-%d %H:%M:%S')
            print(f"   {obj['key']} ({size_str}, {mod_time})")
        if len(group_objects) > 10:
            print(f"   ... è¿˜æœ‰ {len(group_objects) - 10} ä¸ªæ–‡ä»¶")
        print()

def main():
    parser = argparse.ArgumentParser(description='æŸ¥çœ‹ S3 æ•°æ®')
    parser.add_argument('--date', type=str, help='æ—¥æœŸ (YYYY-MM-DD)ï¼Œé»˜è®¤ä»Šå¤©')
    parser.add_argument('--prefix', type=str, default='', help='S3å‰ç¼€è¿‡æ»¤')
    parser.add_argument('--check-dashboard', action='store_true', help='æ£€æŸ¥dashboardéœ€è¦çš„æ•°æ®')
    parser.add_argument('--bucket', type=str, help='S3 bucketåç§°ï¼ˆè¦†ç›–ç¯å¢ƒå˜é‡ï¼‰')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥AWSå‡­è¯
    try:
        session = boto3.Session()
        credentials = session.get_credentials()
        if not credentials:
            print("âŒ AWSå‡­è¯æœªé…ç½®")
            print("   è®¾ç½®ç¯å¢ƒå˜é‡: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY")
            sys.exit(1)
    except Exception as e:
        print(f"âŒ æ£€æŸ¥AWSå‡­è¯å¤±è´¥: {e}")
        sys.exit(1)
    
    # è·å–bucketåç§°
    bucket = args.bucket or os.getenv("AWS_S3_PULSE_BUCKET")
    if not bucket:
        print("âŒ S3 bucketåç§°æœªè®¾ç½®")
        print("   è®¾ç½®ç¯å¢ƒå˜é‡: AWS_S3_PULSE_BUCKET")
        print("   æˆ–ä½¿ç”¨ --bucket å‚æ•°")
        sys.exit(1)
    
    # åˆå§‹åŒ–S3å®¢æˆ·ç«¯
    try:
        region = os.getenv("AWS_REGION", "us-east-2")
        s3_client = boto3.client('s3', region_name=region)
        s3_client.head_bucket(Bucket=bucket)
        print(f"âœ… è¿æ¥åˆ° S3 bucket: {bucket}")
    except ClientError as e:
        error_code = e.response.get('Error', {}).get('Code', '')
        if error_code == '404':
            print(f"âŒ S3 bucket '{bucket}' ä¸å­˜åœ¨")
        else:
            print(f"âŒ æ— æ³•è®¿é—® S3 bucket: {e}")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ è¿æ¥S3å¤±è´¥: {e}")
        sys.exit(1)
    
    # æ‰§è¡Œæ“ä½œ
    if args.check_dashboard:
        date = args.date or datetime.now(timezone.utc).date().isoformat()
        check_dashboard_data(s3_client, bucket, date)
    else:
        list_all_data(s3_client, bucket, args.prefix)

if __name__ == "__main__":
    main()
