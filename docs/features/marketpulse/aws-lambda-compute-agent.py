"""
Market Pulse Compute Agent - Signal Formula

Layer 2: Processing Layer
èŒè´£: è¯»å–åŸå§‹æ•°æ®ï¼Œè®¡ç®— Signal æŒ‡æ ‡
æŠ€æœ¯: AWS Lambda (Python 3.11), EventBridge, boto3, statistics

å…¬å¼:
Return = (Close - Open) / Open
Vol = Std(Return over last 20 bars) + 1e-6
Signal = Return / Vol

å¤„ç†æµç¨‹:
1. è¯»å– raw-data/ (ä» Storage Layer)
2. è®¡ç®— Signal (ä½¿ç”¨æ–°å…¬å¼)
3. å­˜å‚¨ç»“æœ (åˆ° Storage Layer)

è§¦å‘æ–¹å¼:
- EventBridge: æ¯ 5 åˆ†é’Ÿè‡ªåŠ¨è§¦å‘ (è¿‘å®æ—¶)
- æ‰‹åŠ¨è§¦å‘: é€šè¿‡ AWS Console æˆ– CLI

è‚¡ç¥¨åˆ—è¡¨: AAPL, MSFT, AMZN, NVDA, TSLA, META, GOOGL, JPM, XOM, SPY
"""

import json
import boto3
import os
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional
from statistics import stdev
import logging

# é…ç½®æ—¥å¿—
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# åˆå§‹åŒ– AWS å®¢æˆ·ç«¯
s3_client = boto3.client('s3')
BUCKET_NAME = os.environ.get('S3_BUCKET_NAME')

# æ”¯æŒçš„è‚¡ç¥¨åˆ—è¡¨
SUPPORTED_TICKERS = ['AAPL', 'MSFT', 'AMZN', 'NVDA', 'TSLA', 'META', 'GOOGL', 'JPM', 'XOM', 'SPY']


# ============================================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šæ•°æ®è¯»å–
# ============================================================================

def read_raw_data_from_s3(date: str, ticker: str) -> List[Dict[str, Any]]:
    """
    ä» S3 è¯»å–æŒ‡å®šæ—¥æœŸå’Œ ticker çš„åŸå§‹æ•°æ®
    
    Args:
        date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ "YYYY-MM-DD"
        ticker: è‚¡ç¥¨ä»£ç 
    
    Returns:
        List[Dict]: åŸå§‹ bar æ•°æ®åˆ—è¡¨ï¼ˆæŒ‰æ—¶é—´æ’åºï¼‰
    """
    prefix = f"raw-data/{date}/{ticker}/"
    
    logger.info(f"ğŸ“¥ Reading raw data from S3: {prefix}")
    
    bars = []
    
    try:
        # åˆ—å‡ºæ‰€æœ‰å¯¹è±¡
        paginator = s3_client.get_paginator('list_objects_v2')
        pages = paginator.paginate(Bucket=BUCKET_NAME, Prefix=prefix)
        
        for page in pages:
            if 'Contents' not in page:
                continue
            
            for obj in page['Contents']:
                key = obj['Key']
                
                # è¯»å– JSON æ–‡ä»¶
                try:
                    response = s3_client.get_object(Bucket=BUCKET_NAME, Key=key)
                    bar_data = json.loads(response['Body'].read().decode('utf-8'))
                    bars.append(bar_data)
                except Exception as e:
                    logger.warning(f"Failed to read {key}: {e}")
                    continue
        
        # æŒ‰æ—¶é—´æˆ³æ’åº
        bars.sort(key=lambda x: x.get('timestamp', ''))
        
        logger.info(f"âœ… Read {len(bars)} raw bars for {ticker}")
        return bars
        
    except Exception as e:
        logger.error(f"âŒ Error reading raw data for {ticker}: {e}")
        return []


# ============================================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šSignal è®¡ç®—
# ============================================================================

def calculate_signal(bars: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
    """
    è®¡ç®— Signal æŒ‡æ ‡
    
    å…¬å¼:
    Return = (Close - Open) / Open
    Vol = Std(Return over last 20 bars) + 1e-6
    Signal = Return / Vol
    
    Args:
        bars: åŸå§‹ bar æ•°æ®åˆ—è¡¨ï¼ˆè‡³å°‘éœ€è¦ 20 ä¸ª barsï¼‰
    
    Returns:
        Dict: {
            "timestamp": "...",
            "ticker": "...",
            "return": 0.0012,
            "vol": 0.0023,
            "signal": 0.52
        } æˆ– Noneï¼ˆå¦‚æœæ•°æ®ä¸è¶³ï¼‰
    """
    if len(bars) < 20:
        logger.warning(f"Insufficient bars: {len(bars)} < 20")
        return None
    
    # è·å–æœ€æ–° bar
    latest_bar = bars[-1]
    bar_data = latest_bar.get('bar_data', {})
    
    open_price = bar_data.get('open')
    close_price = bar_data.get('close')
    ticker = latest_bar.get('ticker', 'UNKNOWN')
    timestamp = latest_bar.get('timestamp')
    
    if not open_price or not close_price or open_price == 0:
        logger.warning(f"Invalid price data for {ticker}")
        return None
    
    # è®¡ç®—å½“å‰ Return
    current_return = (close_price - open_price) / open_price
    
    # è®¡ç®—è¿‡å» 20 ä¸ª bars çš„ Return åˆ—è¡¨
    returns = []
    for i in range(max(0, len(bars) - 20), len(bars)):
        bar = bars[i]
        bar_data_item = bar.get('bar_data', {})
        bar_open = bar_data_item.get('open')
        bar_close = bar_data_item.get('close')
        
        if bar_open and bar_close and bar_open > 0:
            bar_return = (bar_close - bar_open) / bar_open
            returns.append(bar_return)
    
    if len(returns) < 2:
        logger.warning(f"Insufficient returns for volatility calculation: {len(returns)}")
        return None
    
    # è®¡ç®— Vol (æ ‡å‡†å·® + å°å¸¸æ•°)
    vol = stdev(returns) + 1e-6
    
    # è®¡ç®— Signal
    signal = current_return / vol
    
    return {
        "timestamp": timestamp,
        "ticker": ticker,
        "return": round(current_return, 6),
        "vol": round(vol, 6),
        "signal": round(signal, 4),
        "bars_used": len(returns)
    }


# ============================================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šä¸»å¤„ç†æµç¨‹
# ============================================================================

def process_daily_signals(date: str) -> Dict[str, Any]:
    """
    å¤„ç†æŒ‡å®šæ—¥æœŸçš„æ•°æ®ï¼Œè®¡ç®—æ‰€æœ‰ ticker çš„ Signal
    
    Args:
        date: æ—¥æœŸå­—ç¬¦ä¸² "YYYY-MM-DD"
    
    Returns:
        Dict: å¤„ç†ç»“æœç»Ÿè®¡
    """
    logger.info(f"ğŸš€ Starting signal processing for {date}")
    
    all_signals = []
    
    # å¯¹æ¯ä¸ª ticker è®¡ç®— Signal
    for ticker in SUPPORTED_TICKERS:
        try:
            # è¯»å–åŸå§‹æ•°æ®
            bars = read_raw_data_from_s3(date, ticker)
            
            if len(bars) < 20:
                logger.warning(f"âš ï¸  Insufficient data for {ticker}: {len(bars)} bars")
                continue
            
            # è®¡ç®— Signal
            signal_data = calculate_signal(bars)
            
            if signal_data:
                all_signals.append(signal_data)
                logger.info(f"âœ… Computed signal for {ticker}: {signal_data['signal']:.4f}")
            else:
                logger.warning(f"âš ï¸  Failed to compute signal for {ticker}")
                
        except Exception as e:
            logger.error(f"âŒ Error processing {ticker}: {e}")
            continue
    
    logger.info(f"âœ… Computed {len(all_signals)} signals for {len(SUPPORTED_TICKERS)} tickers")
    
    # å­˜å‚¨ç»“æœåˆ° S3
    processed_prefix = f"processed-data/{date}/"
    
    signals_data = {
        "date": date,
        "processed_at": datetime.now(timezone.utc).isoformat().replace('+00:00', 'Z'),
        "signals": all_signals,
        "tickers_processed": len(all_signals),
        "total_tickers": len(SUPPORTED_TICKERS)
    }
    
    try:
        s3_client.put_object(
            Bucket=BUCKET_NAME,
            Key=f"{processed_prefix}compute-signals.json",
            Body=json.dumps(signals_data, default=str, ensure_ascii=False, indent=2),
            ContentType='application/json'
        )
        logger.info(f"âœ… Stored compute signals to S3")
    except Exception as e:
        logger.error(f"âŒ Error storing signals: {e}")
        raise
    
    return {
        "success": True,
        "date": date,
        "signals_count": len(all_signals),
        "tickers_processed": len(all_signals),
        "total_tickers": len(SUPPORTED_TICKERS)
    }


# ============================================================================
# Lambda Handler (å…¥å£ç‚¹)
# ============================================================================

def lambda_handler(event, context):
    """
    Lambda å‡½æ•°å…¥å£ç‚¹
    
    è§¦å‘æ–¹å¼ï¼š
    1. EventBridge (å®šæ—¶): event åŒ…å« date å­—æ®µ
    2. æ‰‹åŠ¨è§¦å‘: å¯ä»¥ä¼ é€’ date å‚æ•°
    
    Args:
        event: Lambda äº‹ä»¶å¯¹è±¡
        context: Lambda ä¸Šä¸‹æ–‡
    
    Returns:
        Dict: å¤„ç†ç»“æœ
    """
    try:
        # è·å–æ—¥æœŸï¼ˆä» event æˆ–ä½¿ç”¨ä»Šå¤©ï¼‰
        date = event.get('date') or datetime.now(timezone.utc).date().isoformat()
        
        logger.info(f"ğŸ“… Processing Compute Agent signals for {date}")
        logger.info(f"ğŸ“¦ Bucket: {BUCKET_NAME}")
        
        if not BUCKET_NAME:
            raise ValueError("S3_BUCKET_NAME environment variable not set")
        
        # æ‰§è¡Œå¤„ç†
        result = process_daily_signals(date)
        
        logger.info(f"âœ… Processing completed: {result}")
        
        return {
            'statusCode': 200,
            'body': json.dumps(result, default=str)
        }
        
    except Exception as e:
        logger.error(f"âŒ Error processing Compute Agent signals: {e}", exc_info=True)
        
        return {
            'statusCode': 500,
            'body': json.dumps({
                'success': False,
                'error': str(e)
            })
        }
